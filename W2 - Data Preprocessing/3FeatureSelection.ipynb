{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Math, Latex\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "`sklearn.feature_selection` module has useful APIs to select features/reduce dimensionality, either to improve estimators' accuracy score or to boost their performance on very high-dimensional datasets.\n",
    "\n",
    "Top reasons to use feature selection are:\n",
    "\n",
    "\n",
    "* It enables the machine learning algorithm to train faster.\n",
    "\n",
    "* It reduces the complexity of a model and makes it easier to interpret.\n",
    "\n",
    "* It improves the accuracy of a model if the right subset is chosen.\n",
    "\n",
    "* It reduces overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.FILTER-BASED METHODS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.A. Variance Threshold\n",
    "\n",
    "* This transformer helps to keep only high variance features by providing a certain threshold.\n",
    "\n",
    "* Features with  variance greater or equal to threshold value are kept rest are removed.\n",
    "\n",
    "* By default, it removes any feature with same value i.e. 0 variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.25 , 67.735])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [{'age': 4, 'height': 96.0},\n",
    "        {'age': 1, 'height': 73.9},\n",
    "        {'age': 3,  'height': 88.9},\n",
    "        {'age': 2, 'height': 81.6}\n",
    "        ]\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer     \n",
    "dv = DictVectorizer(sparse=False)\n",
    "\n",
    "data_transformed = dv.fit_transform(data)\n",
    "np.var(data_transformed, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[96. ],\n",
       "       [73.9],\n",
       "       [88.9],\n",
       "       [81.6]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "vt = VarianceThreshold(threshold=5)\n",
    "\n",
    "data_new = vt.fit_transform(data_transformed)\n",
    "data_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may observe from output of above cell, the transformer has removed the age feature because its variance is below the threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.B. SelectKBest\n",
    "\n",
    "It selects k-highest scoring features based on a function and removes the rest of the features.\n",
    "\n",
    "Let's take an example of California Housing Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
    "\n",
    "\n",
    "X_california, y_california = fetch_california_housing(return_X_y=True)\n",
    "X, y = X_california[:2000], y_california[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select 3 most important features, since it is a regression problem, we can use only `mutual_info_regression` of `f_regression` scoring functions only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature-matrix before feature selection : (2000, 8)\n",
      "Shape of feature-matrix after feature selection : (2000, 3)\n"
     ]
    }
   ],
   "source": [
    "# mutual_info_regression is scoring method for linear regression method\n",
    "\n",
    "skb = SelectKBest(mutual_info_regression, k=3)\n",
    "X_new = skb.fit_transform(X, y)\n",
    "\n",
    "print(f'Shape of feature-matrix before feature selection : {X.shape}')\n",
    "print(f'Shape of feature-matrix after feature selection : {X_new.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.C. SelectPercentile\n",
    "\n",
    "* This is very similar to `SelectKBest` from previous section, the only difference is, it selects top `percentile` of all features and drops the rest of features.\n",
    "\n",
    "* Similar to `SelecKBest`, it also uses a scoring function to decide the importance of features.\n",
    "\n",
    "Let's use the california housing price dataset for this API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature-matrix before feature selection : (2000, 8)\n",
      "Shape of feature-matrix after feature selection : (2000, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectPercentile\n",
    "\n",
    "sp = SelectPercentile(mutual_info_regression, percentile=30)\n",
    "X_new = sp.fit_transform(X, y)\n",
    "\n",
    "print(f'Shape of feature-matrix before feature selection : {X.shape}')\n",
    "print(f'Shape of feature-matrix after feature selection : {X_new.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from above output, the transformed data now only has top 30 percentile of features, i.e only 3 out of 8 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['x0', 'x6', 'x7'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skb.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.D. GenericUnivariateSelect\n",
    "\n",
    "* It applies  univariate feature selection with a certain strategy, which is passed to the API via `mode` parameter. \n",
    "\n",
    "* The `mode` can take one of the following values : \n",
    "\n",
    "    * `percentile` (top percentage)\n",
    "\n",
    "    * `k_best` (top k)\n",
    "\n",
    "    * `fpr` (false positive rate)\n",
    "\n",
    "    * `fdr` (false discovery rate)\n",
    "\n",
    "    * `fwe` (family wise error rate) \n",
    "\n",
    "* If we want to accomplish the same objective as `SelectKBest`, we can use following code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature-matrix before feature selection : (2000, 8)\n",
      "Shape of feature-matrix after feature selection : (2000, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import GenericUnivariateSelect \n",
    "\n",
    "gus = GenericUnivariateSelect(mutual_info_regression, mode='k_best', param = 3)\n",
    "X_new = gus.fit_transform(X,y)\n",
    "\n",
    "print(f'Shape of feature-matrix before feature selection : {X.shape}')\n",
    "print(f'Shape of feature-matrix after feature selection : {X_new.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.WRAPPER-BASED METHODS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.A. Recursive Feature Elimination (RFE)\n",
    "\n",
    "* STEP 1 : Fits the model \n",
    "\n",
    "* STEP 2 : Ranks the features, afterwards it removes one or more features (depending upn `step` parameter)\n",
    "\n",
    "These two steps are repeated until desired number of features are selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "estimator = LinearRegression()\n",
    "\n",
    "selector = RFE(estimator, n_features_to_select=3, step=3)\n",
    "selector = selector.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True False False False False False  True  True]\n",
      "Rank of each feature is : [1 3 3 2 3 2 1 1]\n"
     ]
    }
   ],
   "source": [
    "# support_ attribute is a boolean array marking which features are selected\n",
    "print(selector.support_)\n",
    "\n",
    "# rank of each feature\n",
    "# if it's value is '1', then it is selected\n",
    "# features with rank 2 and onwards are ranked least.\n",
    "print(f'Rank of each feature is : {selector.ranking_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature-matrix before feature selection : (2000, 8)\n",
      "Shape of feature-matrix after feature selection : (2000, 3)\n"
     ]
    }
   ],
   "source": [
    "X_new = selector.transform(X)\n",
    "\n",
    "print(f'Shape of feature-matrix before feature selection : {X.shape}')\n",
    "print(f'Shape of feature-matrix after feature selection : {X_new.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.B. SelectFromModel\n",
    "\n",
    "* Selects desired number of important features (as specified with `max_features` parameter) above certain threshold of feature importance as obtained from the trained estimator.\n",
    "\n",
    "* The feature importance is obtained via `coef_`, `feature_importance_` or an `importance_getter` callable from the trained estimator.\n",
    "\n",
    "* The feature importance threshold can be specified either numerically or through string argument based on built-in heuristics such as `mean`, `median` and `float` multiples of these like `0.1*mean`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "estimator = LinearRegression()\n",
    "estimator.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients of features :\n",
      " [ 3.64048292e-01  5.56221906e-03  5.13591243e-02 -1.64474348e-01\n",
      "  5.90411479e-05 -1.64573915e-01 -2.17724525e-01 -1.85343265e-01]\n",
      "\n",
      "Intercept of features : -13.720597901356236\n",
      "\n",
      "Indices of top 3 features : [1 2 0]\n"
     ]
    }
   ],
   "source": [
    "print(f'Coefficients of features :\\n {estimator.coef_}')\n",
    "print()\n",
    "print(f'Intercept of features : {estimator.intercept_}')\n",
    "print()\n",
    "print(f'Indices of top {3} features : {np.argsort(estimator.coef_)[-3:]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature-matrix before feature selection : (2000, 8)\n",
      "Shape of feature-matrix after feature selection : (2000, 3)\n"
     ]
    }
   ],
   "source": [
    "t = np.argsort(np.abs(estimator.coef_))[-3:]\n",
    "\n",
    "model = SelectFromModel(estimator, max_features=3, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "\n",
    "print(f'Shape of feature-matrix before feature selection : {X.shape}')\n",
    "print(f'Shape of feature-matrix after feature selection : {X_new.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.C. SequentialFeatureSelection\n",
    "\n",
    "It performs feature selection by selecting or deselecting features one by one in a greedy manner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True False False False False  True  True False]\n",
      "Wall time: 135 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "estimator = LinearRegression()\n",
    "sfs = SequentialFeatureSelector(estimator, n_features_to_select=3)\n",
    "\n",
    "sfs.fit_transform(X, y)\n",
    "print(sfs.get_support())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features corresponding to True in the output of sfs.get_support() are selected. In this case,features 1, 6 and 7 are selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True False False False False  True  True False]\n",
      "Wall time: 194 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "estimator = LinearRegression()\n",
    "sfs = SequentialFeatureSelector(\n",
    "    estimator, n_features_to_select=3, direction='backward')\n",
    "\n",
    "sfs.fit_transform(X, y)\n",
    "print(sfs.get_support())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of observations: \n",
    "* Both `forward` and `backward` selection methods select the same featurers.\n",
    "\n",
    "* The `backward` selection method takes longer than `forward` selection method. \n",
    "\n",
    "From above examples, we can observe that depending upon number of features, `SFS` can accomplish feature selection in different periods forwards and backwards. \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "32f02dc107888b055323539767db1f9cf579f03b0ed3a3ace7986ed2d38433ec"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
